# Retrieval-Augmented-Generation

Retrieval-Augmented Generation (RAG) is a technique in natural language processing (NLP) that combines the strengths of retrieval-based and generation-based models to produce more accurate and contextually relevant responses. Here is a breakdown of RAG:
* **Retrieval** - Seeking relevant information from a source given a query. For example, getting relevant passages of Wikipedia text from a database given a question.
* **Augmentation** - Using the relevant retrieved information to modify an input to a generative model (e.g. an LLM).
* **Generation** - Generating an output given an input. For example, in the case of an LLM, generating a passage of text given an input prompt.

![RAG work flow](https://github.com/patilurjit/Retrieval-Augmented-Generation/blob/main/images/RAG%20Flow.jpg)

For this project, [Human Nutrition: 2020 Edition](https://pressbooks.oer.hawaii.edu/humannutrition2/), a 1200 page text was used to provide context for user queries. The text was downloaded, preprocessed, and used to generate the embeddings for chunks of text which were then stored in a Vector database for faster retrieval.

# Why are RAGs used?
1. **Prevention of hallucinations**: Large language models (LLMs) are impressive, but they can sometimes produce incorrect yet seemingly accurate information. RAG pipelines mitigate this issue by supplying LLMs with factual data from retrieved sources, leading to more accurate outputs. Even if a RAG-generated answer appears incorrect, the retrieval process provides access to the original sources for verification.
2. **Utilizing custom data**: While many base LLMs are trained on vast amounts of internet text, giving them strong language modeling capabilities, they often lack specific, detailed knowledge. RAG systems address this by incorporating domain-specific data, such as medical records or company documentation, enabling LLMs to generate customized and contextually relevant responses for specialized applications.

# Implementation
This repository houses two implementations of Retrieval Augmented Generation:  
### LangChain implementation  
1. The LangChain implementation makes use of the `LangChain` framework with `gpt-3.5-turbo` as the LLM. This RAG pipeline has been developed using LangChain functionalities for a more streamlined implementation, and uses the OpenAI API for all the calls to the LLM and for the generation of embeddings using the `text-embedding-ada-002` model.
2. The knowledge base has been manually cleaned and curated for an improved quality of retrieval, and utilizes **multi-vector** representation of documents for improved retrieval quality. Each document has a corresponding summary that has been generated using the LLM, and stored in the vector database **(Pinecone)**, because, often times the raw, chunked, documents may not be the optimal text to get your embeddings from. The user query is matched with the summaries of the documents to determine which documents, stored in a separate database and which act as the actual knowlwdge base, are to be used as the context for response generation.
3. The LangChain implementation also includes the **multi-query** functionality, where in the user query is passed as an input to the LLM to generate 5 variations of the same query, to enhnace a possibly sub-optimal user query and expand the result set. The aim of multi-query is to have an expanded results sets which might be able to answer questions better than documents retrieved from a single query. These results will be deduplicated (in case the same document comes back multiple times) and then used as context for response generation.

### Custom RAG implementation
1. **Import and format PDF document** - The PDF document is downloaded with a get request and then preprocessed to be used further.
2. **Splitting pages into sentences** - The text on each page is split into individual sentences which are further split into smaller lists of sentences to fit within the context window of the embedding generation model using a `spaCy` pipeline.
3. **Embedding generation** - The embeddings are generated by passing the smaller chunks of text to the `all-mpnet-base-v2` model which generates an embedding vector of size 768.
4. **Vector database for storing embeddings** - The embeddings are stored in MongoDB Atlas, which is a vector database for faster retrieval of relevant text.
5. **Retrieve relevant passages** - Relevant passages are retrieved using the `vectorSearch` operator.
6. **Setting up the LLM model for response generation** - The LLM chosen for response generation is Google's `gemma-2b-it`, which is the instruct version of the Gemma model with 2 billion parameters.
7. **Augmenting the prompt with context items** - The prompt is combined with the context items retrieved from the database.
8. **Combine the retrieval augmentation and response generation** - The prompt is augmented with the retrieved items and used to generate a response.

# Example
![Example 1](https://github.com/patilurjit/Retrieval-Augmented-Generation/blob/main/images/Example%201.png)

# Acknowledgements
Thank you [Daniel Bourke](https://www.youtube.com/@mrdbourke), [Greg Kamradt](https://community.fullstackretrieval.com/), and [Santiago Valdarrama](https://www.youtube.com/@underfitted) for the excellent content that guided my process of creating a RAG with my own customizations.
